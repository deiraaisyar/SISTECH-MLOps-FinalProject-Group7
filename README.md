# Recommendation Engine for Jobs, Courses, Majors, Career, and Job Articles

# Overview
This project builds a comprehensive recommendation engine designed to guide users in exploring suitable jobs, educational courses, university majors, career paths, and job-related articles. By integrating diverse datasets and utilizing text-based similarity techniques, the system helps users navigate through various options based on their interests or background. The goal is to provide a more personalized and informative experience for individuals making academic or career decisions, especially students or fresh graduates. The recommendation results are generated by analyzing both user inputs and content features, ensuring relevant and practical suggestions across different domains.

---
## Features
- Career recommendation based on userâ€™s RIASEC result, interests, and skills
- Job recommendation based on career match
- Course & Certification recommendations based on career match
- Majors & University recommendations based on career match
- Career outlook & articles

---
## Scraping Methods

---
## Vectorization
The vectorization process in this project uses Sentence Transformers, a state-of-the-art model for generating dense vector representations of text. This method is particularly effective for capturing semantic meaning, making it ideal for tasks like recommendation systems. The steps include:

1. **Text Embedding**: Input text data is converted into dense vector representations using the Sentence Transformers model (`all-MiniLM-L6-v2`).
2. **Normalization**: The generated vectors are normalized to unit length to ensure consistency in cosine similarity calculations.
3. **Storage**: The embeddings are stored in a FAISS index, which allows for efficient similarity searches and retrievals.

---
## Vector Database

---
## Similarity

The similarity calculation in this project is based on cosine similarity, a metric that measures the cosine of the angle between two vectors in a multi-dimensional space. This approach is particularly effective for comparing text embeddings generated by models like Sentence Transformers. The steps include:

1. **Embedding Comparison**: The embeddings of the query text and the dataset are compared.
2. **Cosine Similarity Formula**: The similarity score is calculated using the formula:
   \[
   \text{similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}
   \]
   where \(A\) and \(B\) are the embedding vectors.
3. **Ranking**: The results are ranked based on their similarity scores, with higher scores indicating greater relevance.

---
## API

---
## Endpoints
- '/recommend-careers: http://127.0.0.1:8000/recommend-careers
Accepts RIASEC scores and returns career recommendations.
- /recommend-jobs: http://127.0.0.1:8000/recommend-jobs
Accepts a query string and returns job recommendations.
- /recommend-courses: http://127.0.0.1:8000/recommend-courses
Accepts a query string and returns course recommendations.
- /recommend-programs: http://127.0.0.1:8000/recommend-programs
Accepts a query string and returns university major or program recommendations.
- /get-job-articles: http://127.0.0.1:8000/get-job-articles
Accepts a query string and returns job-related articles.
- /health: http://127.0.0.1:8000/health
A health check endpoint to verify the API is running.

---
## Evaluation

The evaluation process in this project compares various methods for generating recommendations based on their relevance to the input query. The methods evaluated include traditional approaches like Bag of Words (BoW) and TF-IDF, as well as modern techniques like Word2Vec, FastText, and Sentence Transformers. The key findings are:

1. **Traditional Methods**:
   - BoW and TF-IDF performed well in identifying relevant results but were limited by their reliance on word frequency and lack of semantic understanding.
   - Both methods occasionally recommended irrelevant results due to the presence of frequently occurring words in the input text.

2. **Modern Methods**:
   - **Word2Vec**: Performed better than traditional methods, especially when using Word Mover's Distance (WMD) for similarity calculations. However, it sometimes introduced less relevant results when using cosine similarity.
   - **FastText**: Delivered similar performance to Word2Vec with WMD but required more computational resources.
   - **Sentence Transformers**: Outperformed all other methods by capturing semantic meaning effectively. It was the only method that consistently excluded irrelevant results, such as courses unrelated to the input context.

3. **Conclusion**:
   - Sentence Transformers with cosine similarity is the best-performing method. Its ability to understand the context and semantics of the input text ensures highly relevant recommendations.
   - To optimize response time, the model and its precomputed embeddings are saved in advance for quick loading during API calls.

---
## Contributors
- Amelia Wibisono
- Deira Aisya Refani